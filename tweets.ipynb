{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "965358e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.core.display import display, HTML\n",
    "import numpy as np\n",
    "import nltk\n",
    "import ssl\n",
    "import glob, string\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from autocorrect import Speller\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_score, make_scorer, f1_score, classification_report,precision_recall_fscore_support\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "display(HTML('<style>.container { width:95% !important; }</style>'))\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6593433c",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cab1ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data/*'\n",
    "def read_datasets(path):\n",
    "    files_list = glob.glob(path)\n",
    "    df_pos, df_neg, df_neu = (pd.DataFrame() for i in range(3))\n",
    "    for file in files_list:\n",
    "        if 'pos' in file.lower():\n",
    "            df_pos = pd.read_csv(file, header=None).T\n",
    "            df_pos['sentiment'] = 'positive'\n",
    "        if 'neg' in file.lower():\n",
    "            df_neg = pd.read_csv(file, header=None).T\n",
    "            df_neg['sentiment'] = 'negative'\n",
    "        if 'neu' in file.lower():\n",
    "            df_neu = pd.read_csv(file, header=None).T\n",
    "            df_neu['sentiment'] = 'neutral'\n",
    "    return pd.concat([df.rename(columns = {0:'text'}) for df in (df_pos, df_neg, df_neu)], ignore_index=True)\n",
    "\n",
    "    \n",
    "df = read_datasets(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1652c7af",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2945c8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text         5\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adac9567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text         0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(subset=['text'], inplace=True)\n",
    "df.reset_index(drop=True)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f14f3275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3868 entries, 0 to 3872\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   text       3868 non-null   object\n",
      " 1   sentiment  3868 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 90.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d076838",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text = df.text.astype(str)\n",
    "df.text = df.text.str.lower()\n",
    "df.sentiment = df.sentiment.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bc7cf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3868 entries, 0 to 3872\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype   \n",
      "---  ------     --------------  -----   \n",
      " 0   text       3868 non-null   object  \n",
      " 1   sentiment  3868 non-null   category\n",
      "dtypes: category(1), object(1)\n",
      "memory usage: 64.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca62cb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/local/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /usr/local/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /usr/local/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d3623a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6df27f84ad4636b4900faaa723bc86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600d95e32c8f4b5fa8a740c9b0fee14b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3680062b9641ccbead777ba5218b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48315d8415449709b8f5b3d21d8a22e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b90df1e4783c4dca9a894ca27dff6285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenization\n",
    "df['tokens'] = df.text.progress_apply(nltk.word_tokenize)\n",
    "# stemming\n",
    "stemmer = MorphAnalyzer()\n",
    "df['stems'] = df.tokens.progress_apply(lambda tokens: [stemmer.parse(token)[0].normal_form for token in tokens])\n",
    "# lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['lemmas'] = df.tokens.progress_apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n",
    "# misspeling\n",
    "s = Speller()\n",
    "df['tokens_misspellings'] = df.tokens.progress_apply(lambda tokens: [s.autocorrect_word(token) for token in tokens])\n",
    "df['lemmas_misspellings'] = df.lemmas.progress_apply(lambda lemmas: [s.autocorrect_word(lemma) for lemma in lemmas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c6df9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = string.punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12fa5528",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = [word for sentense in df.tokens for word in sentense if word not in punct]\n",
    "dictinary = Counter(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66c51ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_custom = sorted(list(set(words_list)))[325:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50a87881",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english') + stop_words_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96ab560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens_cleaned'] = df.tokens.apply(lambda tokens: [token for token in tokens \\\n",
    "                                       if token.isnumeric() != True \\\n",
    "                                    and token in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a51a979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens_cleaned'] = df['tokens_cleaned'].apply(lambda x: np.nan if x == [] else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b00471fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['tokens_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adeda3e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stems</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>tokens_misspellings</th>\n",
       "      <th>lemmas_misspellings</th>\n",
       "      <th>tokens_cleaned</th>\n",
       "      <th>stems_cleaned</th>\n",
       "      <th>lemmas_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>an inspiration in all aspects: fashion</td>\n",
       "      <td>positive</td>\n",
       "      <td>[an, inspiration, in, all, aspects, :, fashion]</td>\n",
       "      <td>[an, inspiration, in, all, aspects, :, fashion]</td>\n",
       "      <td>[an, inspiration, in, all, aspect, :, fashion]</td>\n",
       "      <td>[an, inspiration, in, all, aspects, a, fashion]</td>\n",
       "      <td>[an, inspiration, in, all, aspect, a, fashion]</td>\n",
       "      <td>[an, inspiration, in, all, aspects, fashion]</td>\n",
       "      <td>[an, inspiration, in, all, aspects, fashion]</td>\n",
       "      <td>[an, inspiration, in, all, aspect, fashion]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fitness</td>\n",
       "      <td>positive</td>\n",
       "      <td>[fitness]</td>\n",
       "      <td>[fitness]</td>\n",
       "      <td>[fitness]</td>\n",
       "      <td>[fitness]</td>\n",
       "      <td>[fitness]</td>\n",
       "      <td>[fitness]</td>\n",
       "      <td>[fitness]</td>\n",
       "      <td>[fitness]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>beauty and personality. :)kisses thefashionicon</td>\n",
       "      <td>positive</td>\n",
       "      <td>[beauty, and, personality, ., :, ), kisses, thefashionicon]</td>\n",
       "      <td>[beauty, and, personality, ., :, ), kisses, thefashionicon]</td>\n",
       "      <td>[beauty, and, personality, ., :, ), kiss, thefashionicon]</td>\n",
       "      <td>[beauty, and, personality, a, a, a, kisses, thefashionicon]</td>\n",
       "      <td>[beauty, and, personality, a, a, a, kiss, thefashionicon]</td>\n",
       "      <td>[beauty, and, personality, kisses, thefashionicon]</td>\n",
       "      <td>[beauty, and, personality, kisses, thefashionicon]</td>\n",
       "      <td>[beauty, and, personality, kiss, thefashionicon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apka apna awam ka channel frankline tv aam admi production please visit or likes  share :)fb page :...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[apka, apna, awam, ka, channel, frankline, tv, aam, admi, production, please, visit, or, likes, share, :, ), fb, page, :, ...]</td>\n",
       "      <td>[apka, apna, awam, ka, channel, frankline, tv, aam, admi, production, please, visit, or, likes, share, :, ), fb, page, :, ...]</td>\n",
       "      <td>[apka, apna, awam, ka, channel, frankline, tv, aam, admi, production, please, visit, or, like, share, :, ), fb, page, :, ...]</td>\n",
       "      <td>[aka, ana, away, ka, channel, franklin, tv, aam, admin, production, please, visit, or, likes, share, a, a, fb, page, a, ...]</td>\n",
       "      <td>[aka, ana, away, ka, channel, franklin, tv, aam, admin, production, please, visit, or, like, share, a, a, fb, page, a, ...]</td>\n",
       "      <td>[apka, apna, awam, ka, channel, frankline, tv, aam, admi, production, please, visit, or, likes, share, fb, page]</td>\n",
       "      <td>[apka, apna, awam, ka, channel, frankline, tv, aam, admi, production, please, visit, or, likes, share, fb, page]</td>\n",
       "      <td>[apka, apna, awam, ka, channel, frankline, tv, aam, admi, production, please, visit, or, like, share, fb, page]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>beautiful album from  the greatest unsung guitar genius of our time - and i've met the great backstage</td>\n",
       "      <td>positive</td>\n",
       "      <td>[beautiful, album, from, the, greatest, unsung, guitar, genius, of, our, time, -, and, i, 've, met, the, great, backstage]</td>\n",
       "      <td>[beautiful, album, from, the, greatest, unsung, guitar, genius, of, our, time, -, and, i, 've, met, the, great, backstage]</td>\n",
       "      <td>[beautiful, album, from, the, greatest, unsung, guitar, genius, of, our, time, -, and, i, 've, met, the, great, backstage]</td>\n",
       "      <td>[beautiful, album, from, the, greatest, unsung, guitar, genius, of, our, time, a, and, i, ve, met, the, great, backstage]</td>\n",
       "      <td>[beautiful, album, from, the, greatest, unsung, guitar, genius, of, our, time, a, and, i, ve, met, the, great, backstage]</td>\n",
       "      <td>[beautiful, album, from, the, greatest, unsung, guitar, genius, of, our, time, and, i, met, the, great, backstage]</td>\n",
       "      <td>[beautiful, album, from, the, greatest, unsung, guitar, genius, of, our, time, and, i, met, the, great, backstage]</td>\n",
       "      <td>[beautiful, album, from, the, greatest, unsung, guitar, genius, of, our, time, and, i, met, the, great, backstage]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                     text sentiment                                                                                                                          tokens                                                                                                                           stems                                                                                                                         lemmas                                                                                                           tokens_misspellings                                                                                                          lemmas_misspellings                                                                                                      tokens_cleaned                                                                                                       stems_cleaned                                                                                                      lemmas_cleaned\n",
       "0  an inspiration in all aspects: fashion                                                                  positive  [an, inspiration, in, all, aspects, :, fashion]                                                                                 [an, inspiration, in, all, aspects, :, fashion]                                                                                 [an, inspiration, in, all, aspect, :, fashion]                                                                                 [an, inspiration, in, all, aspects, a, fashion]                                                                               [an, inspiration, in, all, aspect, a, fashion]                                                                               [an, inspiration, in, all, aspects, fashion]                                                                        [an, inspiration, in, all, aspects, fashion]                                                                        [an, inspiration, in, all, aspect, fashion]                                                                       \n",
       "1   fitness                                                                                                positive  [fitness]                                                                                                                       [fitness]                                                                                                                       [fitness]                                                                                                                      [fitness]                                                                                                                     [fitness]                                                                                                                    [fitness]                                                                                                           [fitness]                                                                                                           [fitness]                                                                                                         \n",
       "2   beauty and personality. :)kisses thefashionicon                                                        positive  [beauty, and, personality, ., :, ), kisses, thefashionicon]                                                                     [beauty, and, personality, ., :, ), kisses, thefashionicon]                                                                     [beauty, and, personality, ., :, ), kiss, thefashionicon]                                                                      [beauty, and, personality, a, a, a, kisses, thefashionicon]                                                                   [beauty, and, personality, a, a, a, kiss, thefashionicon]                                                                    [beauty, and, personality, kisses, thefashionicon]                                                                  [beauty, and, personality, kisses, thefashionicon]                                                                  [beauty, and, personality, kiss, thefashionicon]                                                                  \n",
       "3  apka apna awam ka channel frankline tv aam admi production please visit or likes  share :)fb page :...  positive  [apka, apna, awam, ka, channel, frankline, tv, aam, admi, production, please, visit, or, likes, share, :, ), fb, page, :, ...]  [apka, apna, awam, ka, channel, frankline, tv, aam, admi, production, please, visit, or, likes, share, :, ), fb, page, :, ...]  [apka, apna, awam, ka, channel, frankline, tv, aam, admi, production, please, visit, or, like, share, :, ), fb, page, :, ...]  [aka, ana, away, ka, channel, franklin, tv, aam, admin, production, please, visit, or, likes, share, a, a, fb, page, a, ...]  [aka, ana, away, ka, channel, franklin, tv, aam, admin, production, please, visit, or, like, share, a, a, fb, page, a, ...]  [apka, apna, awam, ka, channel, frankline, tv, aam, admi, production, please, visit, or, likes, share, fb, page]    [apka, apna, awam, ka, channel, frankline, tv, aam, admi, production, please, visit, or, likes, share, fb, page]    [apka, apna, awam, ka, channel, frankline, tv, aam, admi, production, please, visit, or, like, share, fb, page]   \n",
       "4  beautiful album from  the greatest unsung guitar genius of our time - and i've met the great backstage  positive  [beautiful, album, from, the, greatest, unsung, guitar, genius, of, our, time, -, and, i, 've, met, the, great, backstage]      [beautiful, album, from, the, greatest, unsung, guitar, genius, of, our, time, -, and, i, 've, met, the, great, backstage]      [beautiful, album, from, the, greatest, unsung, guitar, genius, of, our, time, -, and, i, 've, met, the, great, backstage]     [beautiful, album, from, the, greatest, unsung, guitar, genius, of, our, time, a, and, i, ve, met, the, great, backstage]     [beautiful, album, from, the, greatest, unsung, guitar, genius, of, our, time, a, and, i, ve, met, the, great, backstage]    [beautiful, album, from, the, greatest, unsung, guitar, genius, of, our, time, and, i, met, the, great, backstage]  [beautiful, album, from, the, greatest, unsung, guitar, genius, of, our, time, and, i, met, the, great, backstage]  [beautiful, album, from, the, greatest, unsung, guitar, genius, of, our, time, and, i, met, the, great, backstage]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for cleaned stemming\n",
    "stemmer = MorphAnalyzer()\n",
    "df['stems_cleaned'] = df.tokens_cleaned.apply(lambda tokens:\n",
    "                                              [stemmer.parse(token)[0].normal_form for token in tokens])\n",
    "# for cleaned lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['lemmas_cleaned'] = df.tokens_cleaned.apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942accd4",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd0b017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ccaa091",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_.drop(columns=['sentiment'])\n",
    "y = df_.sentiment\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=21, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e68ab4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(neutral     0.404854\n",
       " positive    0.306149\n",
       " negative    0.288997\n",
       " Name: sentiment, dtype: float64,\n",
       " neutral     0.404916\n",
       " positive    0.306598\n",
       " negative    0.288486\n",
       " Name: sentiment, dtype: float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True), y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767b0506",
   "metadata": {},
   "source": [
    "### defaul pipeline: logreg@accuracy, gridsearch@5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c124031",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_scorer = make_scorer(precision_score, greater_is_better=True,  pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efa16d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(df_.sentiment.unique())\n",
    "def calc_scores(method, y_predict, y_test, lbls, params, acc_score):\n",
    "    params_list = [method]\n",
    "    columns_ = ['method']\n",
    "    for average in ('micro', 'macro', 'weighted'):\n",
    "        scores = precision_recall_fscore_support(y_predict,\n",
    "                                        y_test,\n",
    "                                        labels=lbls,\n",
    "                                        average=average,\n",
    "                                        beta=0.5)\n",
    "        columns_.extend([ f'precision_{average}',\n",
    "                           f'recall_{average}',\n",
    "                           f'f05_{average}'])\n",
    "        scores_ = scores[0], scores[1], scores[2], _\n",
    "        params_list.extend(list(scores_)[:-1])\n",
    "    params_list.append(params)\n",
    "    params_list.append(acc_score)\n",
    "    columns_.append('params')\n",
    "    columns_.append('acc_score')\n",
    "    return form_df(params_list, columns_)\n",
    "\n",
    "def form_df(data, names):\n",
    "    cols = pd.DataFrame(data).T.columns\n",
    "    df = pd.DataFrame(data).T.rename(\n",
    "    columns={k:v for k,v in zip(cols,names)})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec16eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(method, pipeline, parameters):\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train[method], y_train)\n",
    "    y_predict = grid_search.predict(X_test[method])\n",
    "    acc_score = grid_search.score(X_test[method], y_test)\n",
    "    scores = calc_scores(method, y_predict, y_test, labels, grid_search.best_params_, acc_score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fb897f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "                tokenizer=dummy,\n",
    "                preprocessor=dummy,\n",
    "                binary=True,\n",
    "                max_df=1.0,\n",
    "                )  ),\n",
    "    ('logreg', LogisticRegression(max_iter=1500, random_state=21)),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__min_df': (1, 2, 4, 6, 10, 15),\n",
    "    'vect__max_features': (range(1500, 5500, 1000)),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2), (1, 3)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6baaaeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "453e16ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024cfb883b6344d780b74ca615ff5b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "CPU times: user 50.3 s, sys: 4.71 s, total: 55 s\n",
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "methods = ['text', 'tokens', 'stems', \\\n",
    "           'lemmas', 'tokens_misspellings',  'lemmas_misspellings', \\\n",
    "           'tokens_cleaned', 'stems_cleaned', 'lemmas_cleaned']    \n",
    "# p = Process(target=get_model, args=(method,))\n",
    "# with Pool(9) as p:\n",
    "#     r = list(tqdm(p.imap(get_model, methods), total=len(methods)))\n",
    "r = [get_model(method, pipeline, parameters) for method in tqdm(methods)]\n",
    "df = pd.concat(r).set_index('method')\n",
    "result.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f49569d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision_micro</th>\n",
       "      <th>recall_micro</th>\n",
       "      <th>f05_micro</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f05_macro</th>\n",
       "      <th>precision_weighted</th>\n",
       "      <th>recall_weighted</th>\n",
       "      <th>f05_weighted</th>\n",
       "      <th>params</th>\n",
       "      <th>acc_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>0.913325</td>\n",
       "      <td>0.913325</td>\n",
       "      <td>0.913325</td>\n",
       "      <td>0.907062</td>\n",
       "      <td>0.915911</td>\n",
       "      <td>0.908204</td>\n",
       "      <td>0.916451</td>\n",
       "      <td>0.913325</td>\n",
       "      <td>0.915166</td>\n",
       "      <td>{'vect__max_features': 3500, 'vect__min_df': 6, 'vect__ngram_range': (1, 3)}</td>\n",
       "      <td>0.913325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>0.917206</td>\n",
       "      <td>0.917206</td>\n",
       "      <td>0.917206</td>\n",
       "      <td>0.909486</td>\n",
       "      <td>0.924034</td>\n",
       "      <td>0.911203</td>\n",
       "      <td>0.922451</td>\n",
       "      <td>0.917206</td>\n",
       "      <td>0.920112</td>\n",
       "      <td>{'vect__max_features': 4500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1)}</td>\n",
       "      <td>0.917206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stems</th>\n",
       "      <td>0.917206</td>\n",
       "      <td>0.917206</td>\n",
       "      <td>0.917206</td>\n",
       "      <td>0.909486</td>\n",
       "      <td>0.924034</td>\n",
       "      <td>0.911203</td>\n",
       "      <td>0.922451</td>\n",
       "      <td>0.917206</td>\n",
       "      <td>0.920112</td>\n",
       "      <td>{'vect__max_features': 4500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1)}</td>\n",
       "      <td>0.917206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmas</th>\n",
       "      <td>0.912031</td>\n",
       "      <td>0.912031</td>\n",
       "      <td>0.912031</td>\n",
       "      <td>0.904973</td>\n",
       "      <td>0.916584</td>\n",
       "      <td>0.906409</td>\n",
       "      <td>0.916361</td>\n",
       "      <td>0.912031</td>\n",
       "      <td>0.914541</td>\n",
       "      <td>{'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1)}</td>\n",
       "      <td>0.912031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens_misspellings</th>\n",
       "      <td>0.906856</td>\n",
       "      <td>0.906856</td>\n",
       "      <td>0.906856</td>\n",
       "      <td>0.899512</td>\n",
       "      <td>0.91211</td>\n",
       "      <td>0.901034</td>\n",
       "      <td>0.911566</td>\n",
       "      <td>0.906856</td>\n",
       "      <td>0.909549</td>\n",
       "      <td>{'vect__max_features': 3500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1)}</td>\n",
       "      <td>0.906856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmas_misspellings</th>\n",
       "      <td>0.902975</td>\n",
       "      <td>0.902975</td>\n",
       "      <td>0.902975</td>\n",
       "      <td>0.895546</td>\n",
       "      <td>0.906918</td>\n",
       "      <td>0.896956</td>\n",
       "      <td>0.907399</td>\n",
       "      <td>0.902975</td>\n",
       "      <td>0.905581</td>\n",
       "      <td>{'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1)}</td>\n",
       "      <td>0.902975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens_cleaned</th>\n",
       "      <td>0.906856</td>\n",
       "      <td>0.906856</td>\n",
       "      <td>0.906856</td>\n",
       "      <td>0.8996</td>\n",
       "      <td>0.911512</td>\n",
       "      <td>0.901015</td>\n",
       "      <td>0.911572</td>\n",
       "      <td>0.906856</td>\n",
       "      <td>0.909595</td>\n",
       "      <td>{'vect__max_features': 3500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1)}</td>\n",
       "      <td>0.906856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stems_cleaned</th>\n",
       "      <td>0.906856</td>\n",
       "      <td>0.906856</td>\n",
       "      <td>0.906856</td>\n",
       "      <td>0.8996</td>\n",
       "      <td>0.911512</td>\n",
       "      <td>0.901015</td>\n",
       "      <td>0.911572</td>\n",
       "      <td>0.906856</td>\n",
       "      <td>0.909595</td>\n",
       "      <td>{'vect__max_features': 3500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1)}</td>\n",
       "      <td>0.906856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmas_cleaned</th>\n",
       "      <td>0.902975</td>\n",
       "      <td>0.902975</td>\n",
       "      <td>0.902975</td>\n",
       "      <td>0.895546</td>\n",
       "      <td>0.906989</td>\n",
       "      <td>0.896981</td>\n",
       "      <td>0.907349</td>\n",
       "      <td>0.902975</td>\n",
       "      <td>0.905549</td>\n",
       "      <td>{'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1)}</td>\n",
       "      <td>0.902975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    precision_micro recall_micro f05_micro precision_macro recall_macro f05_macro precision_weighted recall_weighted f05_weighted                                                                        params acc_score\n",
       "method                                                                                                                                                                                                                                   \n",
       "text                 0.913325        0.913325     0.913325  0.907062        0.915911     0.908204  0.916451           0.913325        0.915166     {'vect__max_features': 3500, 'vect__min_df': 6, 'vect__ngram_range': (1, 3)}  0.913325\n",
       "tokens               0.917206        0.917206     0.917206  0.909486        0.924034     0.911203  0.922451           0.917206        0.920112     {'vect__max_features': 4500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1)}  0.917206\n",
       "stems                0.917206        0.917206     0.917206  0.909486        0.924034     0.911203  0.922451           0.917206        0.920112     {'vect__max_features': 4500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1)}  0.917206\n",
       "lemmas               0.912031        0.912031     0.912031  0.904973        0.916584     0.906409  0.916361           0.912031        0.914541     {'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1)}  0.912031\n",
       "tokens_misspellings  0.906856        0.906856     0.906856  0.899512        0.91211      0.901034  0.911566           0.906856        0.909549     {'vect__max_features': 3500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1)}  0.906856\n",
       "lemmas_misspellings  0.902975        0.902975     0.902975  0.895546        0.906918     0.896956  0.907399           0.902975        0.905581     {'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1)}  0.902975\n",
       "tokens_cleaned       0.906856        0.906856     0.906856  0.8996          0.911512     0.901015  0.911572           0.906856        0.909595     {'vect__max_features': 3500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1)}  0.906856\n",
       "stems_cleaned        0.906856        0.906856     0.906856  0.8996          0.911512     0.901015  0.911572           0.906856        0.909595     {'vect__max_features': 3500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1)}  0.906856\n",
       "lemmas_cleaned       0.902975        0.902975     0.902975  0.895546        0.906989     0.896981  0.907349           0.902975        0.905549     {'vect__max_features': 2500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1)}  0.902975"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afa6266",
   "metadata": {},
   "source": [
    "### word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "774f5bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "                tokenizer=dummy,\n",
    "                preprocessor=dummy,\n",
    "                binary=False,\n",
    "                )  ),\n",
    "    ('logreg', LogisticRegression(max_iter=1500, random_state=21)),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__min_df': (1, 2, 4, 6, 10, 15),\n",
    "    'vect__max_features': (range(1500, 5500, 1000)),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2), (1, 3)),\n",
    "    'vect__binary': (False, )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5468956d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be010814c486424cb6a99531a58e6390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "CPU times: user 1min 17s, sys: 9.66 s, total: 1min 27s\n",
      "Wall time: 3min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "methods = ['text', 'tokens', 'stems', \\\n",
    "           'lemmas', 'tokens_misspellings',  'lemmas_misspellings', \\\n",
    "           'tokens_cleaned', 'stems_cleaned', 'lemmas_cleaned']    \n",
    "# p = Process(target=get_model, args=(method,))\n",
    "# with Pool(9) as p:\n",
    "#     r = list(tqdm(p.imap(get_model, methods), total=len(methods)))\n",
    "r = [get_model(method, pipeline, parameters) for method in tqdm(methods)]\n",
    "df = pd.concat(r).set_index('method')\n",
    "result.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff755bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision_micro</th>\n",
       "      <th>recall_micro</th>\n",
       "      <th>f05_micro</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f05_macro</th>\n",
       "      <th>precision_weighted</th>\n",
       "      <th>recall_weighted</th>\n",
       "      <th>f05_weighted</th>\n",
       "      <th>params</th>\n",
       "      <th>acc_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>0.899094</td>\n",
       "      <td>0.899094</td>\n",
       "      <td>0.899094</td>\n",
       "      <td>0.893894</td>\n",
       "      <td>0.900012</td>\n",
       "      <td>0.894802</td>\n",
       "      <td>0.901058</td>\n",
       "      <td>0.899094</td>\n",
       "      <td>0.900327</td>\n",
       "      <td>{'vect__binary': False, 'vect__max_features': 3500, 'vect__min_df': 4, 'vect__ngram_range': (1, 3)}</td>\n",
       "      <td>0.899094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>0.913325</td>\n",
       "      <td>0.913325</td>\n",
       "      <td>0.913325</td>\n",
       "      <td>0.905785</td>\n",
       "      <td>0.920012</td>\n",
       "      <td>0.907319</td>\n",
       "      <td>0.919125</td>\n",
       "      <td>0.913325</td>\n",
       "      <td>0.916587</td>\n",
       "      <td>{'vect__binary': False, 'vect__max_features': 1500, 'vect__min_df': 6, 'vect__ngram_range': (1, 2)}</td>\n",
       "      <td>0.913325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stems</th>\n",
       "      <td>0.913325</td>\n",
       "      <td>0.913325</td>\n",
       "      <td>0.913325</td>\n",
       "      <td>0.905785</td>\n",
       "      <td>0.920012</td>\n",
       "      <td>0.907319</td>\n",
       "      <td>0.919125</td>\n",
       "      <td>0.913325</td>\n",
       "      <td>0.916587</td>\n",
       "      <td>{'vect__binary': False, 'vect__max_features': 1500, 'vect__min_df': 6, 'vect__ngram_range': (1, 2)}</td>\n",
       "      <td>0.913325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmas</th>\n",
       "      <td>0.914618</td>\n",
       "      <td>0.914618</td>\n",
       "      <td>0.914618</td>\n",
       "      <td>0.907533</td>\n",
       "      <td>0.919673</td>\n",
       "      <td>0.908959</td>\n",
       "      <td>0.919309</td>\n",
       "      <td>0.914618</td>\n",
       "      <td>0.917308</td>\n",
       "      <td>{'vect__binary': False, 'vect__max_features': 1500, 'vect__min_df': 4, 'vect__ngram_range': (1, 2)}</td>\n",
       "      <td>0.914618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens_misspellings</th>\n",
       "      <td>0.90815</td>\n",
       "      <td>0.90815</td>\n",
       "      <td>0.90815</td>\n",
       "      <td>0.900147</td>\n",
       "      <td>0.914889</td>\n",
       "      <td>0.901822</td>\n",
       "      <td>0.913835</td>\n",
       "      <td>0.90815</td>\n",
       "      <td>0.911327</td>\n",
       "      <td>{'vect__binary': False, 'vect__max_features': 3500, 'vect__min_df': 1, 'vect__ngram_range': (1, 3)}</td>\n",
       "      <td>0.90815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmas_misspellings</th>\n",
       "      <td>0.901682</td>\n",
       "      <td>0.901682</td>\n",
       "      <td>0.901682</td>\n",
       "      <td>0.894139</td>\n",
       "      <td>0.90768</td>\n",
       "      <td>0.895733</td>\n",
       "      <td>0.906839</td>\n",
       "      <td>0.901682</td>\n",
       "      <td>0.904605</td>\n",
       "      <td>{'vect__binary': False, 'vect__max_features': 3500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1)}</td>\n",
       "      <td>0.901682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens_cleaned</th>\n",
       "      <td>0.904269</td>\n",
       "      <td>0.904269</td>\n",
       "      <td>0.904269</td>\n",
       "      <td>0.896269</td>\n",
       "      <td>0.910804</td>\n",
       "      <td>0.89791</td>\n",
       "      <td>0.910067</td>\n",
       "      <td>0.904269</td>\n",
       "      <td>0.907544</td>\n",
       "      <td>{'vect__binary': False, 'vect__max_features': 4500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1)}</td>\n",
       "      <td>0.904269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stems_cleaned</th>\n",
       "      <td>0.904269</td>\n",
       "      <td>0.904269</td>\n",
       "      <td>0.904269</td>\n",
       "      <td>0.896269</td>\n",
       "      <td>0.910804</td>\n",
       "      <td>0.89791</td>\n",
       "      <td>0.910067</td>\n",
       "      <td>0.904269</td>\n",
       "      <td>0.907544</td>\n",
       "      <td>{'vect__binary': False, 'vect__max_features': 4500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1)}</td>\n",
       "      <td>0.904269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmas_cleaned</th>\n",
       "      <td>0.901682</td>\n",
       "      <td>0.901682</td>\n",
       "      <td>0.901682</td>\n",
       "      <td>0.894139</td>\n",
       "      <td>0.906485</td>\n",
       "      <td>0.895617</td>\n",
       "      <td>0.906553</td>\n",
       "      <td>0.901682</td>\n",
       "      <td>0.904509</td>\n",
       "      <td>{'vect__binary': False, 'vect__max_features': 4500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1)}</td>\n",
       "      <td>0.901682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    precision_micro recall_micro f05_micro precision_macro recall_macro f05_macro precision_weighted recall_weighted f05_weighted                                                                                               params acc_score\n",
       "method                                                                                                                                                                                                                                                          \n",
       "text                 0.899094        0.899094     0.899094  0.893894        0.900012     0.894802  0.901058           0.899094        0.900327     {'vect__binary': False, 'vect__max_features': 3500, 'vect__min_df': 4, 'vect__ngram_range': (1, 3)}  0.899094\n",
       "tokens               0.913325        0.913325     0.913325  0.905785        0.920012     0.907319  0.919125           0.913325        0.916587     {'vect__binary': False, 'vect__max_features': 1500, 'vect__min_df': 6, 'vect__ngram_range': (1, 2)}  0.913325\n",
       "stems                0.913325        0.913325     0.913325  0.905785        0.920012     0.907319  0.919125           0.913325        0.916587     {'vect__binary': False, 'vect__max_features': 1500, 'vect__min_df': 6, 'vect__ngram_range': (1, 2)}  0.913325\n",
       "lemmas               0.914618        0.914618     0.914618  0.907533        0.919673     0.908959  0.919309           0.914618        0.917308     {'vect__binary': False, 'vect__max_features': 1500, 'vect__min_df': 4, 'vect__ngram_range': (1, 2)}  0.914618\n",
       "tokens_misspellings  0.90815         0.90815      0.90815   0.900147        0.914889     0.901822  0.913835           0.90815         0.911327     {'vect__binary': False, 'vect__max_features': 3500, 'vect__min_df': 1, 'vect__ngram_range': (1, 3)}  0.90815 \n",
       "lemmas_misspellings  0.901682        0.901682     0.901682  0.894139        0.90768      0.895733  0.906839           0.901682        0.904605     {'vect__binary': False, 'vect__max_features': 3500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1)}  0.901682\n",
       "tokens_cleaned       0.904269        0.904269     0.904269  0.896269        0.910804     0.89791   0.910067           0.904269        0.907544     {'vect__binary': False, 'vect__max_features': 4500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1)}  0.904269\n",
       "stems_cleaned        0.904269        0.904269     0.904269  0.896269        0.910804     0.89791   0.910067           0.904269        0.907544     {'vect__binary': False, 'vect__max_features': 4500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1)}  0.904269\n",
       "lemmas_cleaned       0.901682        0.901682     0.901682  0.894139        0.906485     0.895617  0.906553           0.901682        0.904509     {'vect__binary': False, 'vect__max_features': 4500, 'vect__min_df': 1, 'vect__ngram_range': (1, 1)}  0.901682"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7547dbc8",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb8e1f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "                tokenizer=dummy,\n",
    "                preprocessor=dummy,\n",
    "                )  ),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('logreg', LogisticRegression(max_iter=1500, random_state=21)),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__min_df': (1, 2, 4, 6, 10, 15),\n",
    "#     'vect__max_features': (range(1500, 5500, 1000)),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2), (1, 3)),\n",
    "    'vect__binary': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "025228db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808cf9c13aaf420c88dab95425f91117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "CPU times: user 1min 12s, sys: 5.1 s, total: 1min 17s\n",
      "Wall time: 2min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "methods = ['text', 'tokens', 'stems', \\\n",
    "           'lemmas', 'tokens_misspellings',  'lemmas_misspellings', \\\n",
    "           'tokens_cleaned', 'stems_cleaned', 'lemmas_cleaned']    \n",
    "# p = Process(target=get_model, args=(method,))\n",
    "# with Pool(9) as p:\n",
    "#     r = list(tqdm(p.imap(get_model, methods), total=len(methods)))\n",
    "r = [get_model(method, pipeline, parameters) for method in tqdm(methods)]\n",
    "df = pd.concat(r).set_index('method')\n",
    "result.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f4fb01c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision_micro</th>\n",
       "      <th>recall_micro</th>\n",
       "      <th>f05_micro</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f05_macro</th>\n",
       "      <th>precision_weighted</th>\n",
       "      <th>recall_weighted</th>\n",
       "      <th>f05_weighted</th>\n",
       "      <th>params</th>\n",
       "      <th>acc_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>0.904269</td>\n",
       "      <td>0.904269</td>\n",
       "      <td>0.904269</td>\n",
       "      <td>0.89541</td>\n",
       "      <td>0.915062</td>\n",
       "      <td>0.897522</td>\n",
       "      <td>0.911652</td>\n",
       "      <td>0.904269</td>\n",
       "      <td>0.908196</td>\n",
       "      <td>{'tfidf__norm': 'l2', 'vect__binary': True, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}</td>\n",
       "      <td>0.904269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>0.917206</td>\n",
       "      <td>0.917206</td>\n",
       "      <td>0.917206</td>\n",
       "      <td>0.909498</td>\n",
       "      <td>0.921075</td>\n",
       "      <td>0.910541</td>\n",
       "      <td>0.923096</td>\n",
       "      <td>0.917206</td>\n",
       "      <td>0.920668</td>\n",
       "      <td>{'tfidf__norm': 'l2', 'vect__binary': True, 'vect__min_df': 1, 'vect__ngram_range': (1, 2)}</td>\n",
       "      <td>0.917206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stems</th>\n",
       "      <td>0.917206</td>\n",
       "      <td>0.917206</td>\n",
       "      <td>0.917206</td>\n",
       "      <td>0.909498</td>\n",
       "      <td>0.921075</td>\n",
       "      <td>0.910541</td>\n",
       "      <td>0.923096</td>\n",
       "      <td>0.917206</td>\n",
       "      <td>0.920668</td>\n",
       "      <td>{'tfidf__norm': 'l2', 'vect__binary': True, 'vect__min_df': 1, 'vect__ngram_range': (1, 2)}</td>\n",
       "      <td>0.917206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmas</th>\n",
       "      <td>0.915912</td>\n",
       "      <td>0.915912</td>\n",
       "      <td>0.915912</td>\n",
       "      <td>0.908433</td>\n",
       "      <td>0.920433</td>\n",
       "      <td>0.90959</td>\n",
       "      <td>0.921689</td>\n",
       "      <td>0.915912</td>\n",
       "      <td>0.919288</td>\n",
       "      <td>{'tfidf__norm': 'l2', 'vect__binary': True, 'vect__min_df': 1, 'vect__ngram_range': (1, 2)}</td>\n",
       "      <td>0.915912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens_misspellings</th>\n",
       "      <td>0.906856</td>\n",
       "      <td>0.906856</td>\n",
       "      <td>0.906856</td>\n",
       "      <td>0.8996</td>\n",
       "      <td>0.911512</td>\n",
       "      <td>0.901015</td>\n",
       "      <td>0.911572</td>\n",
       "      <td>0.906856</td>\n",
       "      <td>0.909595</td>\n",
       "      <td>{'tfidf__norm': 'l2', 'vect__binary': True, 'vect__min_df': 6, 'vect__ngram_range': (1, 1)}</td>\n",
       "      <td>0.906856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmas_misspellings</th>\n",
       "      <td>0.910737</td>\n",
       "      <td>0.910737</td>\n",
       "      <td>0.910737</td>\n",
       "      <td>0.903566</td>\n",
       "      <td>0.91563</td>\n",
       "      <td>0.904952</td>\n",
       "      <td>0.915534</td>\n",
       "      <td>0.910737</td>\n",
       "      <td>0.913493</td>\n",
       "      <td>{'tfidf__norm': 'l2', 'vect__binary': True, 'vect__min_df': 1, 'vect__ngram_range': (1, 2)}</td>\n",
       "      <td>0.910737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens_cleaned</th>\n",
       "      <td>0.905563</td>\n",
       "      <td>0.905563</td>\n",
       "      <td>0.905563</td>\n",
       "      <td>0.898194</td>\n",
       "      <td>0.911168</td>\n",
       "      <td>0.899725</td>\n",
       "      <td>0.910614</td>\n",
       "      <td>0.905563</td>\n",
       "      <td>0.908458</td>\n",
       "      <td>{'tfidf__norm': 'l2', 'vect__binary': True, 'vect__min_df': 6, 'vect__ngram_range': (1, 1)}</td>\n",
       "      <td>0.905563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stems_cleaned</th>\n",
       "      <td>0.905563</td>\n",
       "      <td>0.905563</td>\n",
       "      <td>0.905563</td>\n",
       "      <td>0.898194</td>\n",
       "      <td>0.911168</td>\n",
       "      <td>0.899725</td>\n",
       "      <td>0.910614</td>\n",
       "      <td>0.905563</td>\n",
       "      <td>0.908458</td>\n",
       "      <td>{'tfidf__norm': 'l2', 'vect__binary': True, 'vect__min_df': 6, 'vect__ngram_range': (1, 1)}</td>\n",
       "      <td>0.905563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmas_cleaned</th>\n",
       "      <td>0.909444</td>\n",
       "      <td>0.909444</td>\n",
       "      <td>0.909444</td>\n",
       "      <td>0.901642</td>\n",
       "      <td>0.914043</td>\n",
       "      <td>0.903121</td>\n",
       "      <td>0.914418</td>\n",
       "      <td>0.909444</td>\n",
       "      <td>0.912344</td>\n",
       "      <td>{'tfidf__norm': 'l2', 'vect__binary': True, 'vect__min_df': 4, 'vect__ngram_range': (1, 3)}</td>\n",
       "      <td>0.909444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    precision_micro recall_micro f05_micro precision_macro recall_macro f05_macro precision_weighted recall_weighted f05_weighted                                                                                        params acc_score\n",
       "method                                                                                                                                                                                                                                                   \n",
       "text                 0.904269        0.904269     0.904269  0.89541         0.915062     0.897522  0.911652           0.904269        0.908196     {'tfidf__norm': 'l2', 'vect__binary': True, 'vect__min_df': 15, 'vect__ngram_range': (1, 3)}  0.904269\n",
       "tokens               0.917206        0.917206     0.917206  0.909498        0.921075     0.910541  0.923096           0.917206        0.920668     {'tfidf__norm': 'l2', 'vect__binary': True, 'vect__min_df': 1, 'vect__ngram_range': (1, 2)}   0.917206\n",
       "stems                0.917206        0.917206     0.917206  0.909498        0.921075     0.910541  0.923096           0.917206        0.920668     {'tfidf__norm': 'l2', 'vect__binary': True, 'vect__min_df': 1, 'vect__ngram_range': (1, 2)}   0.917206\n",
       "lemmas               0.915912        0.915912     0.915912  0.908433        0.920433     0.90959   0.921689           0.915912        0.919288     {'tfidf__norm': 'l2', 'vect__binary': True, 'vect__min_df': 1, 'vect__ngram_range': (1, 2)}   0.915912\n",
       "tokens_misspellings  0.906856        0.906856     0.906856  0.8996          0.911512     0.901015  0.911572           0.906856        0.909595     {'tfidf__norm': 'l2', 'vect__binary': True, 'vect__min_df': 6, 'vect__ngram_range': (1, 1)}   0.906856\n",
       "lemmas_misspellings  0.910737        0.910737     0.910737  0.903566        0.91563      0.904952  0.915534           0.910737        0.913493     {'tfidf__norm': 'l2', 'vect__binary': True, 'vect__min_df': 1, 'vect__ngram_range': (1, 2)}   0.910737\n",
       "tokens_cleaned       0.905563        0.905563     0.905563  0.898194        0.911168     0.899725  0.910614           0.905563        0.908458     {'tfidf__norm': 'l2', 'vect__binary': True, 'vect__min_df': 6, 'vect__ngram_range': (1, 1)}   0.905563\n",
       "stems_cleaned        0.905563        0.905563     0.905563  0.898194        0.911168     0.899725  0.910614           0.905563        0.908458     {'tfidf__norm': 'l2', 'vect__binary': True, 'vect__min_df': 6, 'vect__ngram_range': (1, 1)}   0.905563\n",
       "lemmas_cleaned       0.909444        0.909444     0.909444  0.901642        0.914043     0.903121  0.914418           0.909444        0.912344     {'tfidf__norm': 'l2', 'vect__binary': True, 'vect__min_df': 4, 'vect__ngram_range': (1, 3)}   0.909444"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3183434d",
   "metadata": {},
   "source": [
    "### TFIDF + SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ddc806bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "                tokenizer=dummy,\n",
    "                preprocessor=dummy,\n",
    "                )  ),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(max_iter=1000)),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "#     'vect__min_df': (1, 2, 4, 6),\n",
    "#     'vect__max_features': (range(1500, 5500, 1000)),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2), (1, 3)),\n",
    "    'vect__binary': (True, False),\n",
    "    'tfidf__norm': ('l2',),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    'clf__max_iter': (100, 150, 250, 500),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a608ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd820435b0d34816b17cda8041d1a04f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "CPU times: user 47.6 s, sys: 1.7 s, total: 49.3 s\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "methods = ['text', 'tokens', 'stems', \\\n",
    "           'lemmas', 'tokens_misspellings',  'lemmas_misspellings', \\\n",
    "           'tokens_cleaned', 'stems_cleaned', 'lemmas_cleaned']    \n",
    "# p = Process(target=get_model, args=(method,))\n",
    "# with Pool(9) as p:\n",
    "#     r = list(tqdm(p.imap(get_model, methods), total=len(methods)))\n",
    "r = [get_model(method, pipeline, parameters) for method in tqdm(methods)]\n",
    "df = pd.concat(r).set_index('method')\n",
    "result.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a5de7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision_micro</th>\n",
       "      <th>recall_micro</th>\n",
       "      <th>f05_micro</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f05_macro</th>\n",
       "      <th>precision_weighted</th>\n",
       "      <th>recall_weighted</th>\n",
       "      <th>f05_weighted</th>\n",
       "      <th>params</th>\n",
       "      <th>acc_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>0.892626</td>\n",
       "      <td>0.892626</td>\n",
       "      <td>0.892626</td>\n",
       "      <td>0.886785</td>\n",
       "      <td>0.891774</td>\n",
       "      <td>0.887094</td>\n",
       "      <td>0.896504</td>\n",
       "      <td>0.892626</td>\n",
       "      <td>0.895092</td>\n",
       "      <td>{'clf__alpha': 1e-05, 'clf__max_iter': 100, 'clf__penalty': 'l2', 'tfidf__norm': 'l2', 'vect__binary': True, 'vect__ngram_range': (1, 3)}</td>\n",
       "      <td>0.892626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>0.918499</td>\n",
       "      <td>0.918499</td>\n",
       "      <td>0.918499</td>\n",
       "      <td>0.912447</td>\n",
       "      <td>0.919058</td>\n",
       "      <td>0.912923</td>\n",
       "      <td>0.922132</td>\n",
       "      <td>0.918499</td>\n",
       "      <td>0.920633</td>\n",
       "      <td>{'clf__alpha': 1e-05, 'clf__max_iter': 150, 'clf__penalty': 'l2', 'tfidf__norm': 'l2', 'vect__binary': True, 'vect__ngram_range': (1, 2)}</td>\n",
       "      <td>0.918499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stems</th>\n",
       "      <td>0.923674</td>\n",
       "      <td>0.923674</td>\n",
       "      <td>0.923674</td>\n",
       "      <td>0.918502</td>\n",
       "      <td>0.92382</td>\n",
       "      <td>0.919102</td>\n",
       "      <td>0.926084</td>\n",
       "      <td>0.923674</td>\n",
       "      <td>0.925155</td>\n",
       "      <td>{'clf__alpha': 1e-05, 'clf__max_iter': 250, 'clf__penalty': 'l2', 'tfidf__norm': 'l2', 'vect__binary': True, 'vect__ngram_range': (1, 3)}</td>\n",
       "      <td>0.923674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmas</th>\n",
       "      <td>0.921087</td>\n",
       "      <td>0.921087</td>\n",
       "      <td>0.921087</td>\n",
       "      <td>0.916031</td>\n",
       "      <td>0.921105</td>\n",
       "      <td>0.916719</td>\n",
       "      <td>0.923232</td>\n",
       "      <td>0.921087</td>\n",
       "      <td>0.922468</td>\n",
       "      <td>{'clf__alpha': 1e-05, 'clf__max_iter': 150, 'clf__penalty': 'l2', 'tfidf__norm': 'l2', 'vect__binary': True, 'vect__ngram_range': (1, 2)}</td>\n",
       "      <td>0.921087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens_misspellings</th>\n",
       "      <td>0.914618</td>\n",
       "      <td>0.914618</td>\n",
       "      <td>0.914618</td>\n",
       "      <td>0.90924</td>\n",
       "      <td>0.915599</td>\n",
       "      <td>0.910199</td>\n",
       "      <td>0.916521</td>\n",
       "      <td>0.914618</td>\n",
       "      <td>0.915803</td>\n",
       "      <td>{'clf__alpha': 1e-05, 'clf__max_iter': 100, 'clf__penalty': 'elasticnet', 'tfidf__norm': 'l2', 'vect__binary': True, 'vect__ngram_range': (1, 2)}</td>\n",
       "      <td>0.914618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmas_misspellings</th>\n",
       "      <td>0.909444</td>\n",
       "      <td>0.909444</td>\n",
       "      <td>0.909444</td>\n",
       "      <td>0.905069</td>\n",
       "      <td>0.90897</td>\n",
       "      <td>0.905534</td>\n",
       "      <td>0.910934</td>\n",
       "      <td>0.909444</td>\n",
       "      <td>0.910337</td>\n",
       "      <td>{'clf__alpha': 1e-05, 'clf__max_iter': 100, 'clf__penalty': 'l2', 'tfidf__norm': 'l2', 'vect__binary': True, 'vect__ngram_range': (1, 3)}</td>\n",
       "      <td>0.909444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens_cleaned</th>\n",
       "      <td>0.921087</td>\n",
       "      <td>0.921087</td>\n",
       "      <td>0.921087</td>\n",
       "      <td>0.916626</td>\n",
       "      <td>0.921351</td>\n",
       "      <td>0.917203</td>\n",
       "      <td>0.922845</td>\n",
       "      <td>0.921087</td>\n",
       "      <td>0.922139</td>\n",
       "      <td>{'clf__alpha': 1e-05, 'clf__max_iter': 250, 'clf__penalty': 'l2', 'tfidf__norm': 'l2', 'vect__binary': True, 'vect__ngram_range': (1, 3)}</td>\n",
       "      <td>0.921087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stems_cleaned</th>\n",
       "      <td>0.904269</td>\n",
       "      <td>0.904269</td>\n",
       "      <td>0.904269</td>\n",
       "      <td>0.899696</td>\n",
       "      <td>0.902254</td>\n",
       "      <td>0.899972</td>\n",
       "      <td>0.905568</td>\n",
       "      <td>0.904269</td>\n",
       "      <td>0.905089</td>\n",
       "      <td>{'clf__alpha': 1e-05, 'clf__max_iter': 150, 'clf__penalty': 'l2', 'tfidf__norm': 'l2', 'vect__binary': True, 'vect__ngram_range': (1, 3)}</td>\n",
       "      <td>0.904269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmas_cleaned</th>\n",
       "      <td>0.901682</td>\n",
       "      <td>0.901682</td>\n",
       "      <td>0.901682</td>\n",
       "      <td>0.898084</td>\n",
       "      <td>0.900114</td>\n",
       "      <td>0.898166</td>\n",
       "      <td>0.903063</td>\n",
       "      <td>0.901682</td>\n",
       "      <td>0.902496</td>\n",
       "      <td>{'clf__alpha': 1e-05, 'clf__max_iter': 100, 'clf__penalty': 'elasticnet', 'tfidf__norm': 'l2', 'vect__binary': True, 'vect__ngram_range': (1, 2)}</td>\n",
       "      <td>0.901682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    precision_micro recall_micro f05_micro precision_macro recall_macro f05_macro precision_weighted recall_weighted f05_weighted                                                                                                                                             params acc_score\n",
       "method                                                                                                                                                                                                                                                                                                        \n",
       "text                 0.892626        0.892626     0.892626  0.886785        0.891774     0.887094  0.896504           0.892626        0.895092     {'clf__alpha': 1e-05, 'clf__max_iter': 100, 'clf__penalty': 'l2', 'tfidf__norm': 'l2', 'vect__binary': True, 'vect__ngram_range': (1, 3)}          0.892626\n",
       "tokens               0.918499        0.918499     0.918499  0.912447        0.919058     0.912923  0.922132           0.918499        0.920633     {'clf__alpha': 1e-05, 'clf__max_iter': 150, 'clf__penalty': 'l2', 'tfidf__norm': 'l2', 'vect__binary': True, 'vect__ngram_range': (1, 2)}          0.918499\n",
       "stems                0.923674        0.923674     0.923674  0.918502        0.92382      0.919102  0.926084           0.923674        0.925155     {'clf__alpha': 1e-05, 'clf__max_iter': 250, 'clf__penalty': 'l2', 'tfidf__norm': 'l2', 'vect__binary': True, 'vect__ngram_range': (1, 3)}          0.923674\n",
       "lemmas               0.921087        0.921087     0.921087  0.916031        0.921105     0.916719  0.923232           0.921087        0.922468     {'clf__alpha': 1e-05, 'clf__max_iter': 150, 'clf__penalty': 'l2', 'tfidf__norm': 'l2', 'vect__binary': True, 'vect__ngram_range': (1, 2)}          0.921087\n",
       "tokens_misspellings  0.914618        0.914618     0.914618  0.90924         0.915599     0.910199  0.916521           0.914618        0.915803     {'clf__alpha': 1e-05, 'clf__max_iter': 100, 'clf__penalty': 'elasticnet', 'tfidf__norm': 'l2', 'vect__binary': True, 'vect__ngram_range': (1, 2)}  0.914618\n",
       "lemmas_misspellings  0.909444        0.909444     0.909444  0.905069        0.90897      0.905534  0.910934           0.909444        0.910337     {'clf__alpha': 1e-05, 'clf__max_iter': 100, 'clf__penalty': 'l2', 'tfidf__norm': 'l2', 'vect__binary': True, 'vect__ngram_range': (1, 3)}          0.909444\n",
       "tokens_cleaned       0.921087        0.921087     0.921087  0.916626        0.921351     0.917203  0.922845           0.921087        0.922139     {'clf__alpha': 1e-05, 'clf__max_iter': 250, 'clf__penalty': 'l2', 'tfidf__norm': 'l2', 'vect__binary': True, 'vect__ngram_range': (1, 3)}          0.921087\n",
       "stems_cleaned        0.904269        0.904269     0.904269  0.899696        0.902254     0.899972  0.905568           0.904269        0.905089     {'clf__alpha': 1e-05, 'clf__max_iter': 150, 'clf__penalty': 'l2', 'tfidf__norm': 'l2', 'vect__binary': True, 'vect__ngram_range': (1, 3)}          0.904269\n",
       "lemmas_cleaned       0.901682        0.901682     0.901682  0.898084        0.900114     0.898166  0.903063           0.901682        0.902496     {'clf__alpha': 1e-05, 'clf__max_iter': 100, 'clf__penalty': 'elasticnet', 'tfidf__norm': 'l2', 'vect__binary': True, 'vect__ngram_range': (1, 2)}  0.901682"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b319710",
   "metadata": {},
   "source": [
    "### TFIDF + DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f333217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "                tokenizer=dummy,\n",
    "                preprocessor=dummy,\n",
    "                )  ),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "#     'vect__min_df': (1, 2, 3),\n",
    "#     'vect__max_features': (range(1500, 5500, 1000)),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "    'vect__binary': (True, False),\n",
    "    'tfidf__norm': ('l2',),\n",
    "    'tree__criterion': (\"gini\", \"entropy\"),\n",
    "    'tree__max_depth': (None, 5, 7, 11, 15),\n",
    "    'tree__max_features': (100, 150, 250, 500),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ed46b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e703573d0584b9cb2af93f651f95a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 160 candidates, totalling 800 fits\n",
      "Fitting 5 folds for each of 160 candidates, totalling 800 fits\n",
      "Fitting 5 folds for each of 160 candidates, totalling 800 fits\n",
      "Fitting 5 folds for each of 160 candidates, totalling 800 fits\n",
      "Fitting 5 folds for each of 160 candidates, totalling 800 fits\n",
      "Fitting 5 folds for each of 160 candidates, totalling 800 fits\n",
      "Fitting 5 folds for each of 160 candidates, totalling 800 fits\n",
      "Fitting 5 folds for each of 160 candidates, totalling 800 fits\n",
      "Fitting 5 folds for each of 160 candidates, totalling 800 fits\n",
      "CPU times: user 1min 8s, sys: 1.53 s, total: 1min 10s\n",
      "Wall time: 2min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "methods = ['text', 'tokens', 'stems', \\\n",
    "           'lemmas', 'tokens_misspellings',  'lemmas_misspellings', \\\n",
    "           'tokens_cleaned', 'stems_cleaned', 'lemmas_cleaned']    \n",
    "# p = Process(target=get_model, args=(method,))\n",
    "# with Pool(9) as p:\n",
    "#     r = list(tqdm(p.imap(get_model, methods), total=len(methods)))\n",
    "r = [get_model(method, pipeline, parameters) for method in tqdm(methods)]\n",
    "df = pd.concat(r).set_index('method')\n",
    "result.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9690a4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision_micro</th>\n",
       "      <th>recall_micro</th>\n",
       "      <th>f05_micro</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f05_macro</th>\n",
       "      <th>precision_weighted</th>\n",
       "      <th>recall_weighted</th>\n",
       "      <th>f05_weighted</th>\n",
       "      <th>params</th>\n",
       "      <th>acc_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>0.851229</td>\n",
       "      <td>0.851229</td>\n",
       "      <td>0.851229</td>\n",
       "      <td>0.844639</td>\n",
       "      <td>0.861938</td>\n",
       "      <td>0.846193</td>\n",
       "      <td>0.857433</td>\n",
       "      <td>0.851229</td>\n",
       "      <td>0.854327</td>\n",
       "      <td>{'tfidf__norm': 'l2', 'tree__criterion': 'gini', 'tree__max_depth': 11, 'tree__max_features': 500, 'vect__binary': True, 'vect__ngram_range': (1, 2)}</td>\n",
       "      <td>0.851229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>0.878396</td>\n",
       "      <td>0.878396</td>\n",
       "      <td>0.878396</td>\n",
       "      <td>0.873174</td>\n",
       "      <td>0.879436</td>\n",
       "      <td>0.87387</td>\n",
       "      <td>0.881194</td>\n",
       "      <td>0.878396</td>\n",
       "      <td>0.880092</td>\n",
       "      <td>{'tfidf__norm': 'l2', 'tree__criterion': 'entropy', 'tree__max_depth': None, 'tree__max_features': 500, 'vect__binary': False, 'vect__ngram_range': (1, 1)}</td>\n",
       "      <td>0.878396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stems</th>\n",
       "      <td>0.886158</td>\n",
       "      <td>0.886158</td>\n",
       "      <td>0.886158</td>\n",
       "      <td>0.879905</td>\n",
       "      <td>0.88916</td>\n",
       "      <td>0.881032</td>\n",
       "      <td>0.88985</td>\n",
       "      <td>0.886158</td>\n",
       "      <td>0.888354</td>\n",
       "      <td>{'tfidf__norm': 'l2', 'tree__criterion': 'gini', 'tree__max_depth': None, 'tree__max_features': 500, 'vect__binary': False, 'vect__ngram_range': (1, 1)}</td>\n",
       "      <td>0.886158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmas</th>\n",
       "      <td>0.839586</td>\n",
       "      <td>0.839586</td>\n",
       "      <td>0.839586</td>\n",
       "      <td>0.836256</td>\n",
       "      <td>0.836328</td>\n",
       "      <td>0.836241</td>\n",
       "      <td>0.840028</td>\n",
       "      <td>0.839586</td>\n",
       "      <td>0.839913</td>\n",
       "      <td>{'tfidf__norm': 'l2', 'tree__criterion': 'gini', 'tree__max_depth': None, 'tree__max_features': 500, 'vect__binary': False, 'vect__ngram_range': (1, 1)}</td>\n",
       "      <td>0.839586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens_misspellings</th>\n",
       "      <td>0.822768</td>\n",
       "      <td>0.822768</td>\n",
       "      <td>0.822768</td>\n",
       "      <td>0.804959</td>\n",
       "      <td>0.834452</td>\n",
       "      <td>0.804377</td>\n",
       "      <td>0.854472</td>\n",
       "      <td>0.822768</td>\n",
       "      <td>0.842086</td>\n",
       "      <td>{'tfidf__norm': 'l2', 'tree__criterion': 'gini', 'tree__max_depth': 15, 'tree__max_features': 500, 'vect__binary': True, 'vect__ngram_range': (1, 1)}</td>\n",
       "      <td>0.822768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmas_misspellings</th>\n",
       "      <td>0.856404</td>\n",
       "      <td>0.856404</td>\n",
       "      <td>0.856404</td>\n",
       "      <td>0.852579</td>\n",
       "      <td>0.855847</td>\n",
       "      <td>0.853117</td>\n",
       "      <td>0.857354</td>\n",
       "      <td>0.856404</td>\n",
       "      <td>0.85704</td>\n",
       "      <td>{'tfidf__norm': 'l2', 'tree__criterion': 'entropy', 'tree__max_depth': None, 'tree__max_features': 500, 'vect__binary': True, 'vect__ngram_range': (1, 1)}</td>\n",
       "      <td>0.856404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens_cleaned</th>\n",
       "      <td>0.874515</td>\n",
       "      <td>0.874515</td>\n",
       "      <td>0.874515</td>\n",
       "      <td>0.867489</td>\n",
       "      <td>0.874986</td>\n",
       "      <td>0.868479</td>\n",
       "      <td>0.877796</td>\n",
       "      <td>0.874515</td>\n",
       "      <td>0.876594</td>\n",
       "      <td>{'tfidf__norm': 'l2', 'tree__criterion': 'gini', 'tree__max_depth': None, 'tree__max_features': 500, 'vect__binary': False, 'vect__ngram_range': (1, 1)}</td>\n",
       "      <td>0.874515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stems_cleaned</th>\n",
       "      <td>0.86934</td>\n",
       "      <td>0.86934</td>\n",
       "      <td>0.86934</td>\n",
       "      <td>0.864771</td>\n",
       "      <td>0.866549</td>\n",
       "      <td>0.865073</td>\n",
       "      <td>0.870086</td>\n",
       "      <td>0.86934</td>\n",
       "      <td>0.86988</td>\n",
       "      <td>{'tfidf__norm': 'l2', 'tree__criterion': 'gini', 'tree__max_depth': None, 'tree__max_features': 500, 'vect__binary': True, 'vect__ngram_range': (1, 1)}</td>\n",
       "      <td>0.86934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmas_cleaned</th>\n",
       "      <td>0.849935</td>\n",
       "      <td>0.849935</td>\n",
       "      <td>0.849935</td>\n",
       "      <td>0.843145</td>\n",
       "      <td>0.848968</td>\n",
       "      <td>0.843827</td>\n",
       "      <td>0.853125</td>\n",
       "      <td>0.849935</td>\n",
       "      <td>0.851997</td>\n",
       "      <td>{'tfidf__norm': 'l2', 'tree__criterion': 'gini', 'tree__max_depth': None, 'tree__max_features': 500, 'vect__binary': False, 'vect__ngram_range': (1, 1)}</td>\n",
       "      <td>0.849935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    precision_micro recall_micro f05_micro precision_macro recall_macro f05_macro precision_weighted recall_weighted f05_weighted                                                                                                                                                       params acc_score\n",
       "method                                                                                                                                                                                                                                                                                                                  \n",
       "text                 0.851229        0.851229     0.851229  0.844639        0.861938     0.846193  0.857433           0.851229        0.854327     {'tfidf__norm': 'l2', 'tree__criterion': 'gini', 'tree__max_depth': 11, 'tree__max_features': 500, 'vect__binary': True, 'vect__ngram_range': (1, 2)}        0.851229\n",
       "tokens               0.878396        0.878396     0.878396  0.873174        0.879436     0.87387   0.881194           0.878396        0.880092     {'tfidf__norm': 'l2', 'tree__criterion': 'entropy', 'tree__max_depth': None, 'tree__max_features': 500, 'vect__binary': False, 'vect__ngram_range': (1, 1)}  0.878396\n",
       "stems                0.886158        0.886158     0.886158  0.879905        0.88916      0.881032  0.88985            0.886158        0.888354     {'tfidf__norm': 'l2', 'tree__criterion': 'gini', 'tree__max_depth': None, 'tree__max_features': 500, 'vect__binary': False, 'vect__ngram_range': (1, 1)}     0.886158\n",
       "lemmas               0.839586        0.839586     0.839586  0.836256        0.836328     0.836241  0.840028           0.839586        0.839913     {'tfidf__norm': 'l2', 'tree__criterion': 'gini', 'tree__max_depth': None, 'tree__max_features': 500, 'vect__binary': False, 'vect__ngram_range': (1, 1)}     0.839586\n",
       "tokens_misspellings  0.822768        0.822768     0.822768  0.804959        0.834452     0.804377  0.854472           0.822768        0.842086     {'tfidf__norm': 'l2', 'tree__criterion': 'gini', 'tree__max_depth': 15, 'tree__max_features': 500, 'vect__binary': True, 'vect__ngram_range': (1, 1)}        0.822768\n",
       "lemmas_misspellings  0.856404        0.856404     0.856404  0.852579        0.855847     0.853117  0.857354           0.856404        0.85704      {'tfidf__norm': 'l2', 'tree__criterion': 'entropy', 'tree__max_depth': None, 'tree__max_features': 500, 'vect__binary': True, 'vect__ngram_range': (1, 1)}   0.856404\n",
       "tokens_cleaned       0.874515        0.874515     0.874515  0.867489        0.874986     0.868479  0.877796           0.874515        0.876594     {'tfidf__norm': 'l2', 'tree__criterion': 'gini', 'tree__max_depth': None, 'tree__max_features': 500, 'vect__binary': False, 'vect__ngram_range': (1, 1)}     0.874515\n",
       "stems_cleaned        0.86934         0.86934      0.86934   0.864771        0.866549     0.865073  0.870086           0.86934         0.86988      {'tfidf__norm': 'l2', 'tree__criterion': 'gini', 'tree__max_depth': None, 'tree__max_features': 500, 'vect__binary': True, 'vect__ngram_range': (1, 1)}      0.86934 \n",
       "lemmas_cleaned       0.849935        0.849935     0.849935  0.843145        0.848968     0.843827  0.853125           0.849935        0.851997     {'tfidf__norm': 'l2', 'tree__criterion': 'gini', 'tree__max_depth': None, 'tree__max_features': 500, 'vect__binary': False, 'vect__ngram_range': (1, 1)}     0.849935"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a2cb21",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0ccbb85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=df_.tokens.tolist(), window=16, min_count=1, size=150, iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5dfe293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_train = []\n",
    "for index, row in X_train.iterrows():\n",
    "#     print(row)\n",
    "    model_vector = (np.mean([model.wv[token] for token in row['tokens']], axis=0)).tolist()\n",
    "#     if isinstance(model_vector, float):\n",
    "#         print(row)\n",
    "    vectors_train.append(model_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64df8908",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_test = []\n",
    "for index, row in X_test.iterrows():\n",
    "    model_vector = (np.mean([model.wv[token] for token in row['tokens']], axis=0)).tolist()\n",
    "    vectors_test.append(model_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "217cf6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logres = LogisticRegression(n_jobs=-1, max_iter=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "45757a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1500, n_jobs=-1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logres.fit(vectors_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df284bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9120310478654593"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, logres.predict(vectors_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48123db7",
   "metadata": {},
   "source": [
    "### FASTTEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18ca5e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "014494a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial = 1\n",
      "epoch = 5\n",
      "lr = 0.1\n",
      "dim = 100\n",
      "minCount = 1\n",
      "wordNgrams = 1\n",
      "minn = 0\n",
      "maxn = 0\n",
      "bucket = 0\n",
      "dsub = 2\n",
      "loss = softmax\n",
      "currentScore = 0.893375\n",
      "train took = 0.163794\n",
      "Trial = 2\n",
      "epoch = 1\n",
      "lr = 0.835603\n",
      "dim = 382\n",
      "minCount = 1\n",
      "wordNgrams = 1\n",
      "minn = 0\n",
      "maxn = 0\n",
      "bucket = 0\n",
      "dsub = 2\n",
      "loss = softmax\n",
      "currentScore = 0.887164\n",
      "train took = 0.199815\n",
      "Trial = 3\n",
      "epoch = 3\n",
      "lr = 0.216041\n",
      "dim = 168\n",
      "minCount = 1\n",
      "wordNgrams = 4\n",
      "minn = 0\n",
      "maxn = 0\n",
      "bucket = 2620593\n",
      "dsub = 2\n",
      "loss = softmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   0.8% Trials:    3 Best score:  0.893375 ETA:   0h 3m18s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currentScore = 0.881988\n",
      "train took = 1.39533\n",
      "Trial = 4\n",
      "epoch = 2\n",
      "lr = 0.984649\n",
      "dim = 23\n",
      "minCount = 1\n",
      "wordNgrams = 1\n",
      "minn = 0\n",
      "maxn = 0\n",
      "bucket = 0\n",
      "dsub = 2\n",
      "loss = softmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress:   1.0% Trials:    4 Best score:  0.893375 ETA:   0h 3m17s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currentScore = 0.89234\n",
      "train took = 0.379419\n",
      "Trial = 5\n",
      "epoch = 56\n",
      "lr = 0.0399362\n",
      "dim = 196\n",
      "minCount = 1\n",
      "wordNgrams = 1\n",
      "minn = 0\n",
      "maxn = 0\n",
      "bucket = 0\n",
      "dsub = 2\n",
      "loss = softmax\n",
      "currentScore = 0.899586\n",
      "train took = 0.250831\n",
      "Trial = 6\n",
      "epoch = 60\n",
      "lr = 0.0212034\n",
      "dim = 152\n",
      "minCount = 1\n",
      "wordNgrams = 5\n",
      "minn = 0\n",
      "maxn = 0\n",
      "bucket = 287917\n",
      "dsub = 2\n",
      "loss = softmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress:   1.3% Trials:    6 Best score:  0.899586 ETA:   0h 3m17s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currentScore = 0.895445\n",
      "train took = 0.592363\n",
      "Trial = 7\n",
      "epoch = 100\n",
      "lr = 0.01\n",
      "dim = 40\n",
      "minCount = 1\n",
      "wordNgrams = 1\n",
      "minn = 0\n",
      "maxn = 0\n",
      "bucket = 0\n",
      "dsub = 2\n",
      "loss = softmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress:   1.5% Trials:    7 Best score:  0.899586 ETA:   0h 3m16s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currentScore = 0.900621\n",
      "train took = 0.270432\n",
      "Trial = 8\n",
      "epoch = 9\n",
      "lr = 0.0185386\n",
      "dim = 33\n",
      "minCount = 1\n",
      "wordNgrams = 1\n",
      "minn = 3\n",
      "maxn = 6\n",
      "bucket = 377970\n",
      "dsub = 2\n",
      "loss = softmax\n",
      "currentScore = 0.796066\n",
      "train took = 0.13969\n",
      "Trial = 9\n",
      "epoch = 24\n",
      "lr = 0.01\n",
      "dim = 12\n",
      "minCount = 1\n",
      "wordNgrams = 3\n",
      "minn = 0\n",
      "maxn = 0\n",
      "bucket = 372933\n",
      "dsub = 4\n",
      "loss = softmax\n",
      "currentScore = 0.812629\n",
      "train took = 0.108451\n",
      "Trial = 10\n",
      "epoch = 100\n",
      "lr = 0.01\n",
      "dim = 27\n",
      "minCount = 1\n",
      "wordNgrams = 3\n",
      "minn = 0\n",
      "maxn = 0\n",
      "bucket = 10000000\n",
      "dsub = 2\n",
      "loss = softmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   2.0% Trials:   10 Best score:  0.900621 ETA:   0h 3m15s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currentScore = 0.899586\n",
      "train took = 0.986851\n",
      "Trial = 11\n",
      "epoch = 33\n",
      "lr = 0.01\n",
      "dim = 63\n",
      "minCount = 1\n",
      "wordNgrams = 2\n",
      "minn = 0\n",
      "maxn = 0\n",
      "bucket = 4521359\n",
      "dsub = 2\n",
      "loss = softmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   2.5% Trials:   11 Best score:  0.900621 ETA:   0h 3m14s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currentScore = 0.874741\n",
      "train took = 1.04783\n",
      "Trial = 12\n",
      "epoch = 100\n",
      "lr = 0.059155\n",
      "dim = 121\n",
      "minCount = 1\n",
      "wordNgrams = 1\n",
      "minn = 3\n",
      "maxn = 6\n",
      "bucket = 3417103\n",
      "dsub = 8\n",
      "loss = softmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   3.8% Trials:   12 Best score:  0.900621 ETA:   0h 3m12s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currentScore = 0.901656\n",
      "train took = 2.40503\n",
      "Trial = 13\n",
      "epoch = 31\n",
      "lr = 0.0486025\n",
      "dim = 73\n",
      "minCount = 1\n",
      "wordNgrams = 1\n",
      "minn = 0\n",
      "maxn = 0\n",
      "bucket = 0\n",
      "dsub = 4\n",
      "loss = softmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress:   4.0% Trials:   13 Best score:  0.901656 ETA:   0h 3m11s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currentScore = 0.900621\n",
      "train took = 0.32014\n",
      "Trial = 14\n",
      "epoch = 21\n",
      "lr = 0.0866757\n",
      "dim = 40\n",
      "minCount = 1\n",
      "wordNgrams = 1\n",
      "minn = 3\n",
      "maxn = 6\n",
      "bucket = 561631\n",
      "dsub = 2\n",
      "loss = softmax\n",
      "currentScore = 0.900621\n",
      "train took = 0.189021\n",
      "Trial = 15\n",
      "epoch = 34\n",
      "lr = 0.569388\n",
      "dim = 293\n",
      "minCount = 1\n",
      "wordNgrams = 2\n",
      "minn = 3\n",
      "maxn = 6\n",
      "bucket = 4367960\n",
      "dsub = 2\n",
      "loss = softmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   6.3% Trials:   15 Best score:  0.901656 ETA:   0h 3m 7s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currentScore = 0.903727\n",
      "train took = 4.24135\n",
      "Trial = 16\n",
      "epoch = 6\n",
      "lr = 0.878024\n",
      "dim = 105\n",
      "minCount = 1\n",
      "wordNgrams = 1\n",
      "minn = 3\n",
      "maxn = 6\n",
      "bucket = 2671003\n",
      "dsub = 2\n",
      "loss = softmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   7.0% Trials:   16 Best score:  0.903727 ETA:   0h 3m 5s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currentScore = 0.901656\n",
      "train took = 1.39947\n",
      "Trial = 17\n",
      "epoch = 67\n",
      "lr = 1.2809\n",
      "dim = 163\n",
      "minCount = 1\n",
      "wordNgrams = 1\n",
      "minn = 3\n",
      "maxn = 6\n",
      "bucket = 674349\n",
      "dsub = 2\n",
      "loss = softmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   7.5% Trials:   17 Best score:  0.903727 ETA:   0h 3m 4s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currentScore = 0.902692\n",
      "train took = 1.34392\n",
      "Trial = 18\n",
      "epoch = 100\n",
      "lr = 5\n",
      "dim = 169\n",
      "minCount = 1\n",
      "wordNgrams = 2\n",
      "minn = 3\n",
      "maxn = 6\n",
      "bucket = 601846\n",
      "dsub = 2\n",
      "loss = softmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   8.5% Trials:   18 Best score:  0.903727 ETA:   0h 3m 2s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currentScore = 0.881988\n",
      "train took = 2.03619\n",
      "Trial = 19\n",
      "epoch = 85\n",
      "lr = 0.503359\n",
      "dim = 147\n",
      "minCount = 1\n",
      "wordNgrams = 5\n",
      "minn = 0\n",
      "maxn = 0\n",
      "bucket = 1750121\n",
      "dsub = 4\n",
      "loss = softmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   9.3% Trials:   19 Best score:  0.903727 ETA:   0h 3m 1s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currentScore = 0.902692\n",
      "train took = 1.23992\n",
      "Trial = 20\n",
      "epoch = 100\n",
      "lr = 1.31014\n",
      "dim = 277\n",
      "minCount = 1\n",
      "wordNgrams = 1\n",
      "minn = 0\n",
      "maxn = 0\n",
      "bucket = 0\n",
      "dsub = 4\n",
      "loss = softmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress:   9.6% Trials:   20 Best score:  0.903727 ETA:   0h 3m 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currentScore = 0.89234\n",
      "train took = 0.462599\n",
      "Trial = 21\n",
      "epoch = 12\n",
      "lr = 0.367235\n",
      "dim = 839\n",
      "minCount = 1\n",
      "wordNgrams = 4\n",
      "minn = 0\n",
      "maxn = 0\n",
      "bucket = 3123296\n",
      "dsub = 2\n",
      "loss = softmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  14.3% Trials:   21 Best score:  0.903727 ETA:   0h 2m51s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currentScore = 0.899586\n",
      "train took = 9.93591\n",
      "Trial = 22\n",
      "epoch = 100\n",
      "lr = 0.214882\n",
      "dim = 367\n",
      "minCount = 1\n",
      "wordNgrams = 1\n",
      "minn = 0\n",
      "maxn = 0\n",
      "bucket = 0\n",
      "dsub = 2\n",
      "loss = softmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  15.3% Trials:   22 Best score:  0.903727 ETA:   0h 2m49s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currentScore = 0.897516\n",
      "train took = 1.86571\n",
      "Trial = 23\n",
      "epoch = 55\n",
      "lr = 0.560999\n",
      "dim = 555\n",
      "minCount = 1\n",
      "wordNgrams = 3\n",
      "minn = 2\n",
      "maxn = 5\n",
      "bucket = 6327541\n",
      "dsub = 2\n",
      "loss = softmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  28.7% Trials:   23 Best score:  0.903727 ETA:   0h 2m22s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currentScore = 0.909938\n",
      "train took = 26.3501\n",
      "Trial = 24\n",
      "epoch = 100\n",
      "lr = 0.168795\n",
      "dim = 639\n",
      "minCount = 1\n",
      "wordNgrams = 1\n",
      "minn = 3\n",
      "maxn = 6\n",
      "bucket = 10000000\n",
      "dsub = 2\n",
      "loss = softmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  64.0% Trials:   24 Best score:  0.909938 ETA:   0h 1m11s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currentScore = 0.902692\n",
      "train took = 71.1544\n",
      "Trial = 25\n",
      "epoch = 42\n",
      "lr = 0.182415\n",
      "dim = 724\n",
      "minCount = 1\n",
      "wordNgrams = 5\n",
      "minn = 3\n",
      "maxn = 6\n",
      "bucket = 7308332\n",
      "dsub = 2\n",
      "loss = softmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% Trials:   25 Best score:  0.909938 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best selected args = 0\n",
      "epoch = 55\n",
      "lr = 0.560999\n",
      "dim = 555\n",
      "minCount = 1\n",
      "wordNgrams = 3\n",
      "minn = 2\n",
      "maxn = 5\n",
      "bucket = 6327541\n",
      "dsub = 2\n",
      "loss = softmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  6394\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread:    7649 lr:  0.000000 avg.loss:  0.029301 ETA:   0h 0m 0s 25.4% words/sec/thread:    2269 lr:  0.418471 avg.loss:  0.106753 ETA:   0h 0m51s\n"
     ]
    }
   ],
   "source": [
    "method = 'text'\n",
    "X = df_[method]\n",
    "y = df_.sentiment\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=21)\n",
    "(X_train + ' __label__' + y_train.astype(str)).to_csv('train', index=False)\n",
    "(X_test + ' __label__' + y_test.astype(str)).to_csv('test', index=False)\n",
    "model = fasttext.train_supervised(input=\"train\", autotuneValidationFile='test', verbose=10000, autotuneDuration=200)\n",
    "acc_score = model.test(\"test\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6d4cbfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9037267080745341"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e02bf2",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4fe30389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2bd385b9eb444659c90f7f6bd8e41da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3863 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = []\n",
    "for i, words1 in enumerate(tqdm(df_[method].values)):\n",
    "    words1 = set(words1)\n",
    "    for j, words2 in enumerate(df_[method].values):\n",
    "        if j <= i:\n",
    "            continue\n",
    "        words2 = set(words2)\n",
    "        inter = len(words1 & words2)\n",
    "        outer = len(words1 | words2)\n",
    "        result.append([inter/outer, i, j])\n",
    "popular = list(filter(lambda x: x[0] < 1, sorted(result, key=lambda x: x[0], reverse=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38b02363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.9629629629629629, 2597, 3049],\n",
       " [0.9615384615384616, 3, 3317],\n",
       " [0.9615384615384616, 54, 174],\n",
       " [0.9615384615384616, 96, 3521],\n",
       " [0.9615384615384616, 115, 3521],\n",
       " [0.9615384615384616, 174, 972],\n",
       " [0.9615384615384616, 828, 874],\n",
       " [0.9615384615384616, 828, 1688],\n",
       " [0.9615384615384616, 828, 1832],\n",
       " [0.9615384615384616, 944, 1688]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2637a4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet1</th>\n",
       "      <th>tweet2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apka apna awam ka channel frankline tv aam admi production please visit or likes  share :)fb page :...</td>\n",
       "      <td>controlling and more. also in epaper.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how sword-wielding processions left gaping.</td>\n",
       "      <td>gurgaon and bengal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we will be never ready actually happy  idhuellam dialogue ku dhan othu varum..</td>\n",
       "      <td>just please look at how scared and full of tence lauren's face is after tryin reaching camila's hand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>never give up on the things that make you smile happy</td>\n",
       "      <td>left few months in hinhua ler unhappy  i need to gambateh wink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>never give up on the things that make you smile happy</td>\n",
       "      <td>just please look at how scared and full of tence lauren's face is after tryin reaching camila's hand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>we will be never ready actually happy  idhuellam dialogue ku dhan othu varum..</td>\n",
       "      <td>that would be a great trick happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>we will be never ready actually happy  idhuellam dialogue ku dhan othu varum..</td>\n",
       "      <td>left few months in hinhua ler unhappy  i need to gambateh wink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ministers of pmln are happy like those students who never studied for exams  luckily exams got delayed. they'll still ha</td>\n",
       "      <td>delhi airport drops after glitches</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wow i just made a js compressor that actually checks scoped variables existing. turns out i have a few issues happy  linters may be useful happy</td>\n",
       "      <td>i'll like her happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jin was spotted at everland today. nice to see them having a break after the us tour happy</td>\n",
       "      <td>wow i just made a js compressor that actually checks scoped variables existing. turns out i have a few issues happy  linters may be useful happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                             tweet1                                                                                                                                            tweet2\n",
       "0  apka apna awam ka channel frankline tv aam admi production please visit or likes  share :)fb page :...                                             controlling and more. also in epaper.                                                                                                          \n",
       "1  how sword-wielding processions left gaping.                                                                                                        gurgaon and bengal.                                                                                                                            \n",
       "2  we will be never ready actually happy  idhuellam dialogue ku dhan othu varum..                                                                    just please look at how scared and full of tence lauren's face is after tryin reaching camila's hand                                            \n",
       "3  never give up on the things that make you smile happy                                                                                             left few months in hinhua ler unhappy  i need to gambateh wink                                                                                  \n",
       "4  never give up on the things that make you smile happy                                                                                             just please look at how scared and full of tence lauren's face is after tryin reaching camila's hand                                            \n",
       "5  we will be never ready actually happy  idhuellam dialogue ku dhan othu varum..                                                                    that would be a great trick happy                                                                                                               \n",
       "6  we will be never ready actually happy  idhuellam dialogue ku dhan othu varum..                                                                    left few months in hinhua ler unhappy  i need to gambateh wink                                                                                  \n",
       "7  ministers of pmln are happy like those students who never studied for exams  luckily exams got delayed. they'll still ha                          delhi airport drops after glitches                                                                                                              \n",
       "8  wow i just made a js compressor that actually checks scoped variables existing. turns out i have a few issues happy  linters may be useful happy   i'll like her happy                                                                                                                            \n",
       "9   jin was spotted at everland today. nice to see them having a break after the us tour happy                                                       wow i just made a js compressor that actually checks scoped variables existing. turns out i have a few issues happy  linters may be useful happy"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = set()\n",
    "for _, i, j in popular:\n",
    "    df.add(tuple(df_['text'][[i, j]].tolist()))\n",
    "    if len(df) >= 10:\n",
    "        break\n",
    "        \n",
    "df = pd.DataFrame(df, columns=['tweet1', 'tweet2'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8a986d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ad5196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9f903e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b08d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856e2f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f56771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f5bb96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34daa28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
